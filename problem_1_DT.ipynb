{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1h-7NtZx42Q"
   },
   "source": [
    "# Problem 1: Decision Trees (DTs) and Random Forests\n",
    "In this question, you are asked to build a decision tree from scratch. You can use evaluation metrics from sklearn, but cannot use the decision tree package from sklearn.\n",
    "\n",
    "Decision trees consists of nodes, each containing a set of data samples. \n",
    "During training, the child nodes are created by splitting the node's data samples based on the values of a certain feature.\n",
    "During testing, the test data sample is passed through the tree until it hits a leaf node, where its label is predicted as the label of the majority of the data samples in that leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRutIhVVxs2F"
   },
   "source": [
    "## Load the Apple Quality Dataset\n",
    "\n",
    "We will use the apple quality dataset for this problem. \n",
    "Load the dataset (given in `data/archive.zip`) into two numpy arrays X and Y.\n",
    "\n",
    "- X should be a 4000*7 numpy array (4000 apples and 7 attributes: size, weight, sweetness, crunshiness, juiciness, ripeness, acidity)\n",
    "- Y should be a 4000*1 numpy array (with value 1 for good quality and 0 for bad quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNuNyRTRxqRQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = os.path.abspath(\"data/apple_quality.zip\")\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Remove the last row ðŸ™‚\n",
    "df = df.iloc[:-1]\n",
    "df[[\"Acidity\"]] = df[[\"Acidity\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Create the arrays X, Y from the dataframe\n",
    "# ===== YOUR CODE HERE =====\n",
    "\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhA8p4nZYJvK"
   },
   "source": [
    "Split the data into training and test data with a ratio of 0.3 (2800 data samples in the training set and 1200 data samples in the test set):\n",
    "\n",
    "- X_train should be a 2800*7 numpy array\n",
    "- Y_train should be a 2800*1 numpy array\n",
    "- X_test should be a 1200*7 numpy array\n",
    "- Y_test should be a 1200*1 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEWF9igrXoYg"
   },
   "outputs": [],
   "source": [
    "# ===== YOUR CODE HERE =====\n",
    "\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK814pakrdvG"
   },
   "source": [
    "## Quality metrics\n",
    "\n",
    "Splitting a node when training a DT is done based on certain quality metrics.\n",
    "There are multiple popular quality metrics, but for this assignment, we only consider **information gain** and **Gini impurity**.\n",
    "\n",
    "### Information gain \n",
    "\n",
    "Information gain is the difference between the entropy of the parent node and the weighted average of the entropy of child nodes.\n",
    "\n",
    "Entropy is given by the following formula:\n",
    "\n",
    "$E=-\\Sigma_{i=1}p_i \\log_2 p_i$\n",
    "\n",
    "where $p_i$ is the class probability, i.e., out of all samples in the node, what fraction belongs to the class $i$.\n",
    "Lower entropy implies that the classes are well divided, which is our goal. \n",
    "Hence, we try to maximize the information gain.\n",
    "\n",
    "### Gini impurity\n",
    "\n",
    "Gini impurity is given by the following formula:\n",
    "\n",
    "$G = \\Sigma_{i=1}p_i(1-p_i)=1-\\Sigma_{i=1}p_i^2$\n",
    "\n",
    "where $p_i$ is once again the class probability.\n",
    "Notice that lower Gini impurity implies that the classes are well divided. \n",
    "Hence, we try to maximize the Gini impurity decrease, i.e., the difference between Gini impurity of the parent node and weighted average of the Gini impurity of the child nodes.\n",
    "\n",
    "For simplicity in implementation, we will use the same function for both information gain and Gini impurity, using the entropy or the Gini impurity based on an argument. \n",
    "Assume that the weights for taking the weighted average is the ratio of the number of data samples in the corresponding child node and the number of data samples in the parent node.\n",
    "\n",
    "The basic structure of code is shown below and you need to finish the code according to the structure. \n",
    "Do not delete/change the names of the pre-defined functions since they are used in grading. \n",
    "You can add functions as you need. \n",
    "You can only use numpy and math library to implement these two metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Re8M_XaJrJoF"
   },
   "outputs": [],
   "source": [
    "def entropy(labels_list: np.array) -> float:\n",
    "    \"\"\"Returns the entropy given a list of labels.\n",
    "\n",
    "    Args:\n",
    "        labels_list: Numpy array containing labels.\n",
    "\n",
    "    Returns:\n",
    "        Entropy.\n",
    "    \"\"\"\n",
    "    # ===== YOUR CODE HERE =====\n",
    "\n",
    "    # ==========================\n",
    "\n",
    "\n",
    "def gini_impurity(labels_list: np.array) -> float:\n",
    "    \"\"\"Returns the Gini impurity given a list of labels.\n",
    "\n",
    "    Args:\n",
    "        labels_list: Numpy array containing labels.\n",
    "\n",
    "    Returns:\n",
    "        Gini impurity.\n",
    "    \"\"\"\n",
    "    # ===== YOUR CODE HERE =====\n",
    "\n",
    "    # ==========================\n",
    "\n",
    "\n",
    "def information_gain(\n",
    "    parent_labels: np.array,\n",
    "    l_child_labels: np.array,\n",
    "    r_child_labels: np.array,\n",
    "    mode: str,\n",
    ") -> float:\n",
    "    \"\"\"Returns the information gain or the Gini impurity decrease given a list of labels.\n",
    "\n",
    "    Args:\n",
    "        parent_labels: Numpy array containing labels in the parent node.\n",
    "        l_child_labels: Numpy array containing labels in the left child node.\n",
    "        r_child_labels: Numpy array containing labels in the right child node.\n",
    "        mode: Either \"entropy\" or \"gini\" to select between entropy or Gini impurity.\n",
    "\n",
    "    Returns:\n",
    "        Information gain or the Gini impurity decrease based on the mode.\n",
    "    \"\"\"\n",
    "    # ===== YOUR CODE HERE =====\n",
    "\n",
    "    # =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ_LDSYnrNc2"
   },
   "source": [
    "## Finding the best split \n",
    "\n",
    "During training, a leaf node in the decision tree can be split based on any of the input features at any threshold.\n",
    "However, we want to find the best split in terms of the quality metrics defined above (highest information gain or Gini impurity decrease).\n",
    "\n",
    "For this assignment, let us use a brute force method and loop through all features and all possible thresholds to find the best possible split.\n",
    "\n",
    "Fill the code below to return the best split in the form of a dictionary (see the code comments for the format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx3mnMYQ3eqv"
   },
   "outputs": [],
   "source": [
    "def find_best_split(dataset: np.array, num_features: int, split_mode: str) -> dict:\n",
    "    \"\"\"Find the best split according to the given quality metric.\n",
    "\n",
    "    Args:\n",
    "        dataset: Numpy array of shape (N, k + 1), where N is the number of data samples\n",
    "            and k is the number of features. First k columns contain the features and the\n",
    "            last column contains the label.\n",
    "        num_features: Number of features.\n",
    "        split_mode: Either \"entropy\" or \"gini\" to select between entropy or Gini impurity\n",
    "            while calculating the quality metric.\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the best split. It should have the following keys:\n",
    "            - \"feature_index\": Index of the feature to split on.\n",
    "            - \"threshold\": Threhold of the feature value to split on.\n",
    "            - \"dataset_left\": Left side of the split dataset (feature value < threshold).\n",
    "            - \"dataset_right\": Right side of the split dataset (feature value >=\n",
    "                threshold).\n",
    "            - \"info_gain\": Information gain or Gini impurity decrease (select the mode\n",
    "                based on the value of split_mode).\n",
    "    \"\"\"\n",
    "\n",
    "    def _split(data: np.array, feature_idx: int, feature_threshold: float) -> tuple:\n",
    "        \"\"\"Splits the data into two sets based on the value of the feature at feature_idx\n",
    "        and the threshold given by feature_threshold.\"\"\"\n",
    "\n",
    "        mask_below_threshold = data[:, feature_idx] < feature_threshold\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "\n",
    "    best_split = {\n",
    "        \"feature_index\": None,\n",
    "        \"threshold\": None,\n",
    "        \"dataset_left\": None,\n",
    "        \"dataset_right\": None,\n",
    "        \"info_gain\": 0,\n",
    "    }\n",
    "    max_info_gain = -float(\"inf\")\n",
    "\n",
    "    # Loop over all the features.\n",
    "    for feature_index in range(num_features):\n",
    "        feature_values = dataset[:, feature_index]\n",
    "        possible_threshold = np.unique(feature_values)\n",
    "\n",
    "        # Loop over all the feature values present in the data.\n",
    "        for threshold in possible_threshold:\n",
    "            ...\n",
    "            # 1. Get the current split.\n",
    "            # 2. Enusre that both splits are not empty.\n",
    "            # 3. Compute information gain.\n",
    "            # 4. Update the best split if information gain is higher.\n",
    "\n",
    "            # ===== YOUR CODE HERE =====\n",
    "\n",
    "            # ==========================\n",
    "\n",
    "    # Return best split\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T6flqSzBoAH"
   },
   "source": [
    "## Building the tree\n",
    "\n",
    "Finally, we recursively build the tree starting from the root:\n",
    "\n",
    "- If the stopping conditions are not met, find the best split at the root and recursively call the `build_tree` function from the left and the right child nodes.\n",
    "\n",
    "- Keep splitting until the stopping conditions are met. In this assignment, we have two stopping conditions: the depth of the tree reaches `MAX_DEPTH` or the number of data samples in the current node is less than `MIN_SAMPLE_SPLIT`. You can adjust these two parameters.\n",
    "\n",
    "The tree should use the `Node` class given below. \n",
    "Leaf nodes should at least have the `value` field filled, while decision nodes should have all fields except `value` filled (see the code comments for what they should contain). \n",
    "The `build_tree` function should return the filled root (when called recursively, root here is the root of the sub-tree) node of the decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWC3IjlNAwHC"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import Self\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Node class for decision tree.\n",
    "\n",
    "    Args:\n",
    "        feature_index: (only required for decision nodes) Index of the best split feature.\n",
    "        feature_threshold: (only required for decision nodes) Threshold value of the best\n",
    "            split feature.\n",
    "        left: (only required for decision nodes) Left child node.\n",
    "        right: (only required for decision nodes) Right child node.\n",
    "        info_gain: (only required for decision nodes) The information gain obtained by the\n",
    "            best split on this node.\n",
    "        value: (only required for leaf nodes) The class label to predict if a test sample\n",
    "            reaches this leaf node (equal to the class label of the majority of the data\n",
    "            samples in this leaf node).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_index: int = None,\n",
    "        feature_threshold: float = None,\n",
    "        left: Self = None,\n",
    "        right: Self = None,\n",
    "        info_gain: float = None,\n",
    "        value: int = None,\n",
    "    ):\n",
    "        # For decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = feature_threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "\n",
    "        # For leaf node\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "# Hyperparameters.\n",
    "MAX_DEPTH = 100\n",
    "MIN_SAMPLE_SPLIT = 3\n",
    "\n",
    "\n",
    "def build_tree(dataset: np.array, split_mode: str, curr_depth: int = 0) -> Node:\n",
    "    \"\"\"Build a decision tree given a training dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: Numpy array of shape (N, k + 1), where N is the number of data samples\n",
    "            and k is the number of features. First k columns contain the features and the\n",
    "            last column contains the label.\n",
    "        split_mode: Either \"entropy\" or \"gini\" to select between entropy or Gini impurity\n",
    "            while calculating the quality metric.\n",
    "        curr_depth: Current depth of the tree (starts from 0 and goes up by 1 when\n",
    "            recursively calling this function on child nodes).\n",
    "\n",
    "    Returns:\n",
    "        The root node of the decision tree (in the form of a Node class).\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_leaf_value(Y: np.array) -> int:\n",
    "        \"\"\"Function to compute the value of leaf node.\n",
    "\n",
    "        Args:\n",
    "            Y: Array of labels.\n",
    "        \"\"\"\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "\n",
    "    # 1. Preprocess the dataset if needed\n",
    "    # 2. Split until stopping conditions are met\n",
    "    # 2.1 Find the best split\n",
    "    # 2.2 If information gain is positive,\n",
    "    # 2.2.1 Recur left\n",
    "    # 2.2.2 Recur right\n",
    "    # 2.2.3 Return the filled decision node (in the form of a Node class)\n",
    "\n",
    "    # 3. If the function reaches this point, it means that this node is a leaf node.\n",
    "    #    Calculate the value.\n",
    "    # 4. Return the filled leaf node (in the form of a Node class)\n",
    "\n",
    "    # ===== YOUR CODE HERE =====\n",
    "\n",
    "    # =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The code below trains a decision tree using the `build_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X: np.array, Y: np.array, split_mode: str) -> Node:\n",
    "    \"\"\"Train a decision tree.\n",
    "\n",
    "    Args:\n",
    "        X: Array containing the training inputs.\n",
    "        Y: Array containing the training labels.\n",
    "        split_mode: Either \"entropy\" or \"gini\" to select between entropy or Gini impurity\n",
    "            while calculating the quality metric.\n",
    "\n",
    "    Returns:\n",
    "        Root node of a trained decision tree.\n",
    "    \"\"\"\n",
    "    dataset = np.concatenate((X, Y), axis=1)\n",
    "    root = build_tree(dataset, split_mode)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyDkmigbGYJL"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "`predict` function given below predicts the class labels of given test inputs.\n",
    "Fill the `make_prediction` function, which recursively takes a single data sample down the decision tree and predicts the class label given by the leaf node it encounters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oHM7jDyL8QP"
   },
   "outputs": [],
   "source": [
    "def predict(X: np.array, tree: Node) -> np.array:\n",
    "    \"\"\"Predict the labels with a decision tree.\n",
    "\n",
    "    Args:\n",
    "        X: Array containing the test inputs.\n",
    "        tree: The root node of a trained decision tree.\n",
    "\n",
    "    Returns:\n",
    "        The class labels corresponding to the test inputs.\n",
    "    \"\"\"\n",
    "    predictions = np.array([make_prediction(x, tree) for x in X])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def make_prediction(x: np.array, tree: Node) -> int:\n",
    "    \"\"\"Predict the label of a single data point.\n",
    "\n",
    "    Args:\n",
    "        x: A single test input (array of size num_features)\n",
    "        tree: The root node of a decision sub-tree (hint: you may need to call this\n",
    "            function recursively to find the label)\n",
    "\n",
    "    Returns:\n",
    "        The class label corresponding to the test input.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    # Hint: you need to recursively call this function based on the split feature and\n",
    "    # threshold at the current node till you reach a leaf node.\n",
    "\n",
    "    # ===== YOUR CODE HERE =====\n",
    "\n",
    "    # =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the following:\n",
    "\n",
    "1. The tree structure (you can use the given `print_tree` function)\n",
    "2. Accuracy and F1 score on test set for both information gain and gini impurity decrease. The format should be:\n",
    "```    \n",
    "Information gain\n",
    "    accuracy\n",
    "    F1_score\n",
    "Gini impurity decrease\n",
    "    accuracy\n",
    "    F1_score\n",
    "```\n",
    "3. The final max_depth and min_leaf_node you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bylmLfsbN7LE",
    "outputId": "bc18e33c-2df8-47fb-cd04-efd83c9a5c73"
   },
   "outputs": [],
   "source": [
    "def print_tree(tree, indent=\" \"):\n",
    "    \"\"\"function to print the tree\"\"\"\n",
    "    feature_map = {\n",
    "        0: \"Size\",\n",
    "        1: \"Weight\",\n",
    "        2: \"Sweetness\",\n",
    "        3: \"Crunchiness\",\n",
    "        4: \"Juiciness\",\n",
    "        5: \"Ripeness\",\n",
    "        6: \"Acidity\",\n",
    "    }\n",
    "    if tree.value is not None:\n",
    "        print(tree.value)\n",
    "\n",
    "    else:\n",
    "        print(\"X_\" + feature_map[tree.feature_index], \"<=\", tree.threshold, \"?\")\n",
    "        print(\"%sleft:\" % (indent), end=\"\")\n",
    "        print_tree(tree.left, indent + indent)\n",
    "        print(\"%sright:\" % (indent), end=\"\")\n",
    "        print_tree(tree.right, indent + indent)\n",
    "\n",
    "\n",
    "# Train the decision tree using `train` function and X_train, Y_train as the training\n",
    "# dataset.\n",
    "# Predict the labels for X_test and compute the accuracy, F1 score w.r.t. Y_test.\n",
    "# Print the information asked in the above question.\n",
    "# ===== YOUR CODE HERE =====\n",
    "\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGMG5_2yAhb"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "Decision trees (DTs) can be prone to overfitting, especially when they are deep. To mitigate this, we can use an ensemble method where we combine the predictions of multiple decisions trees and take an average or do majority voting to get our final prediction.\n",
    "\n",
    "We will assume there are $m$ DTs and train them independently, similar to the previous problem. A popular method to train base learners in an ensemble method is via bagging ([bootstrap aggregating](https://en.wikipedia.org/wiki/Bootstrap_aggregating)). Given a training dataset of size $N$, bagging generates $m$ training datasets, each of size $N'$. The training samples in the new datasets are obtained by sampling from the original training set with replacement.\n",
    "\n",
    "## Bagging to get dataset for base DTs\n",
    "\n",
    "First, let us write a function to generate the training datasets for the base DTs with bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o8bFdzp8_ts"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def create_bags(\n",
    "    train_input: np.array, train_label: np.array, num_bags: int, bag_size: int\n",
    ") -> List[tuple]:\n",
    "    \"\"\"Applies bagging to generate num_bags datasets.\n",
    "\n",
    "    Args:\n",
    "        train_input: Inputs in the complete training dataset.\n",
    "        train_label: Labels in the complete training dataset.\n",
    "        num_bags: Number of datasets to create.\n",
    "        bag_size: Size of the smaller datasets.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples such that the tuple at index i contains the X, Y for bag i.\n",
    "    \"\"\"\n",
    "    bags = []\n",
    "\n",
    "    for i in range(num_bags):\n",
    "        ...\n",
    "        # Sample with replacement and add them to a bag.\n",
    "        # ===== YOUR CODE HERE =====\n",
    "\n",
    "        # ==========================\n",
    "\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gH82K3lChAY"
   },
   "source": [
    "## Training the base DTs\n",
    "\n",
    "Let us now write a function to train the base DTs.\n",
    "\n",
    "1. First, use the `create_bags` function to create datasets for the base DTs. Note that the number of bags should be equal to the number of DTs.\n",
    "1. Since we already have DTs implemented, pass the corresponding datasets to the `train` function to train $m$ of them and return the list of trained DTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHZbOIubEAMH"
   },
   "outputs": [],
   "source": [
    "def train_base_dts(\n",
    "    train_input: np.array, train_label: np.array, num_dts: int, bag_size: int\n",
    ") -> List:\n",
    "    \"\"\"Trains num_dts decision trees with training dataset.\n",
    "\n",
    "    Args:\n",
    "        train_input: Inputs in the complete training dataset.\n",
    "        train_label: Labels in the complete training dataset.\n",
    "        num_dts: Number of decision trees.\n",
    "        bag_size: Size of the datasets used to train each decision tree.\n",
    "\n",
    "    Returns:\n",
    "        List of trained decision trees.\n",
    "    \"\"\"\n",
    "    base_dts = []\n",
    "\n",
    "    bags = create_bags(train_input, train_label, num_dts, bag_size)\n",
    "    for i in range(num_dts):\n",
    "        ...\n",
    "        # Train individual DTs with the `train` method from the decision tree problem and\n",
    "        # add them to the base_dts list.\n",
    "        # ===== YOUR CODE HERE =====\n",
    "\n",
    "        # ==========================\n",
    "\n",
    "    return base_dts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdnBJZ3LJKtO"
   },
   "source": [
    "## Predictions via majority voting\n",
    "\n",
    "Once we have trained the base decision trees, we can combine their individual predictions in different ways. For this assignment, let us implement majority voting. The idea here is to predict the class that is predicted by most number of decision trees (i.e., the mode of the $m$ predictions from base decision trees). If there are multiple modes, assume that ties are broken arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2x4odZHkJDpU"
   },
   "outputs": [],
   "source": [
    "def predict_majority_voting(test_input: np.array, trained_base_dts: List) -> np.array:\n",
    "    \"\"\"Predicts the class labels based on majority voting.\n",
    "\n",
    "    Args:\n",
    "        test_input: Inputs in the test dataset.\n",
    "        trained_base_dts: List of trained base decision trees.\n",
    "\n",
    "    Returns:\n",
    "        Array containing the predicted class labels for all test inputs.\n",
    "    \"\"\"\n",
    "    # ===== YOUR CODE HERE =====\n",
    "\n",
    "    # =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYbKw4yTKrAC"
   },
   "source": [
    "## Analyzing the random forest predictions\n",
    "\n",
    "Let us apply the two functions that we have written: `train_base_dts`, and `predict_majority_voting` to train a random forest on the training data from the apple quality dataset and make predictions on the test set (use `X_train`, `Y_train`, `X_test`, `Y_test` created in the decision tree problem).\n",
    "\n",
    "Print the accuracy and F1 score on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMZ5hRRnpUya",
    "outputId": "3f852ef2-8f3a-418a-defd-81c289e95639"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters. You can try different values for these.\n",
    "NUM_DTS = 5  # Number of decision trees.\n",
    "BAG_SIZE = len(X_train)  # Size of the individual datasets for base decision trees.\n",
    "\n",
    "# Call the train_base_dts to train a random forest.\n",
    "# Call predict_majority_voting functions to get the predictions.\n",
    "# Print the accuracy and F1 score (using either information gain or Gini impurity decrease\n",
    "# as the quality metric).\n",
    "# ===== YOUR CODE HERE =====\n",
    "\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Complete the required code and run ALL the code cells in this notebook. \n",
    "Submit the files mentioned in the README."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
